{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5h8H8LriWa"
      },
      "source": [
        "**ASSIGNMENT-5**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ko6UodorbuX"
      },
      "source": [
        "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
        "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
        "parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeGStn7aqI71",
        "outputId": "5d7eddeb-10fb-48ca-c9c7-614251be5952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       lr     alpha status  final_cost  mse_test   r2_test  n_iters\n",
            "10  0.001   1.00000     ok    0.120167  0.219727  0.989340    20000\n",
            "9   0.001   0.10000     ok    0.117177  0.219742  0.989339    20000\n",
            "8   0.001   0.01000     ok    0.116878  0.219744  0.989339    20000\n",
            "7   0.001   0.00100     ok    0.116848  0.219744  0.989339    20000\n",
            "6   0.001   0.00001     ok    0.116844  0.219744  0.989339    20000\n",
            "16  0.010   1.00000     ok    0.120081  0.219756  0.989338    20000\n",
            "11  0.001  10.00000     ok    0.149976  0.219757  0.989338    20000\n",
            "17  0.010  10.00000     ok    0.149970  0.219761  0.989338     8772\n",
            "23  0.100  10.00000     ok    0.149970  0.219762  0.989338     1336\n",
            "15  0.010   0.10000     ok    0.117040  0.219779  0.989337    20000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------- data example (replace with your data) -------------\n",
        "np.random.seed(0)\n",
        "n = 500\n",
        "z = np.random.randn(n,1)\n",
        "X = np.hstack([z + 0.01*np.random.randn(n,1) for _ in range(8)])  # 8 correlated features\n",
        "X = np.hstack([X, np.random.randn(n,2)])  # +2 random features\n",
        "true_w = np.array([2.5, -1.2, 1.8, 0.0, 0.7, -0.5, 1.0, 0.3, 0.0, 0.0])\n",
        "y = X.dot(true_w) + 0.5*np.random.randn(n)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# ------------- preprocessing -------------\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "# ------------- safe ridge by gradient descent -------------\n",
        "def ridge_cost_and_grad(w, X, y, alpha):\n",
        "    m = X.shape[0]\n",
        "    Xb = np.hstack([np.ones((m,1)), X])\n",
        "    preds = Xb.dot(w)\n",
        "    error = preds - y\n",
        "    cost = (1.0/(2*m)) * np.sum(error**2) + (alpha/(2*m)) * np.sum(w[1:]**2)\n",
        "    grad = (1.0/m) * Xb.T.dot(error)\n",
        "    grad[1:] += (alpha/m) * w[1:]\n",
        "    return cost, grad\n",
        "\n",
        "def ridge_gd_safe(X, y, alpha=1.0, lr=1e-3, n_iter=20000, grad_clip=1e3, tol=1e-9, verbose=False):\n",
        "    m, d = X.shape\n",
        "    w = np.zeros(d+1, dtype=np.float64)\n",
        "    costs = []\n",
        "    for i in range(n_iter):\n",
        "        cost, grad = ridge_cost_and_grad(w, X, y, alpha)\n",
        "        # detect bad cost\n",
        "        if not np.isfinite(cost):\n",
        "            if verbose: print(f\"Aborting: cost became non-finite at iter {i}, cost={cost}\")\n",
        "            return None, costs, False\n",
        "        # gradient clipping to avoid single huge jump\n",
        "        grad = np.clip(grad, -grad_clip, grad_clip)\n",
        "        w = w - lr * grad\n",
        "        costs.append(cost)\n",
        "        # detect NaN/Inf in weights\n",
        "        if not np.all(np.isfinite(w)):\n",
        "            if verbose: print(f\"Aborting: weights became non-finite at iter {i}\")\n",
        "            return None, costs, False\n",
        "        # simple convergence check\n",
        "        if i>0 and abs(costs[-2] - costs[-1]) < tol:\n",
        "            return w, costs, True\n",
        "    return w, costs, True\n",
        "\n",
        "# ------------- hyperparameter sweep (safe defaults) -------------\n",
        "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1]            # avoid large lrs like 1, 10\n",
        "alphas = [1e-5, 1e-3, 0.01, 0.1, 1, 10]             # common ridge alphas\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for alpha in alphas:\n",
        "        w, costs, ok = ridge_gd_safe(X_train_s, y_train, alpha=alpha, lr=lr,\n",
        "                                    n_iter=20000, grad_clip=1e4, tol=1e-10, verbose=False)\n",
        "        if not ok or w is None:\n",
        "            # mark as failed (didn't converge or exploded)\n",
        "            results.append({'lr': lr, 'alpha': alpha, 'status': 'failed'})\n",
        "            continue\n",
        "        # evaluate\n",
        "        Xb_test = np.hstack([np.ones((X_test_s.shape[0],1)), X_test_s])\n",
        "        preds = Xb_test.dot(w)\n",
        "        if not np.all(np.isfinite(preds)):\n",
        "            results.append({'lr': lr, 'alpha': alpha, 'status': 'failed_preds'})\n",
        "            continue\n",
        "        mse = mean_squared_error(y_test, preds)\n",
        "        r2  = r2_score(y_test, preds)\n",
        "        results.append({'lr': lr, 'alpha': alpha, 'status': 'ok', 'final_cost': costs[-1],\n",
        "                        'mse_test': mse, 'r2_test': r2, 'n_iters': len(costs)})\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "print(res_df.sort_values(['status','r2_test'], ascending=[True, False]).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DQpUqYwTq8f"
      },
      "source": [
        "Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
        "\n",
        "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "\n",
        "(b) Separate input and output features and perform scaling\n",
        "\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
        "regularization parameter as 0.5748) regression function on the dataset.\n",
        "\n",
        "(d) Evaluate the performance of each trained model on test set. Which model\n",
        "performs the best and Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVjRVTnNO1np",
        "outputId": "dc471674-cc7f-4450-efb6-54074bd32689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7x/dgpwvvh950z1lnsr3h1b_bbh0000gn/T/ipykernel_77708/2290581097.py:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  df = pd.read_csv(csv_url, delim_whitespace=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(322, 21)\n",
            "          Unnamed: 0  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
            "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
            "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
            "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
            "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
            "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
            "\n",
            "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
            "0     66  ...     30    29      14       A        E     446       33      20   \n",
            "1    835  ...    321   414     375       N        W     632       43      10   \n",
            "2    457  ...    224   266     263       A        W     880       82      14   \n",
            "3   1575  ...    828   838     354       N        E     200       11       3   \n",
            "4    101  ...     48    46      33       N        E     805       40       4   \n",
            "\n",
            "   Salary  NewLeague  \n",
            "0     NaN          A  \n",
            "1   475.0          N  \n",
            "2   480.0          A  \n",
            "3   500.0          N  \n",
            "4    91.5          N  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "Categorical columns: ['League', 'Division', 'NewLeague']\n",
            "Linear: RMSE=358.168, R2=0.291\n",
            "Ridge: RMSE=355.646, R2=0.301\n",
            "Lasso: RMSE=355.595, R2=0.301\n"
          ]
        }
      ],
      "source": [
        "# Q2_hitters.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --- A) Load data ---\n",
        "# Option 1: raw gist url (public CSV). Use whichever copy you prefer.\n",
        "csv_url = \"https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/Hitters.csv\"\n",
        "df = pd.read_csv(csv_url, delim_whitespace=False)\n",
        "# The CSV may have the first column as names item with a leading '-'; adjust as needed\n",
        "# If salary has 'NA' strings, pandas will parse them as NaN.\n",
        "\n",
        "# Quick look\n",
        "print(df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# (a) Preprocess: handle nulls, noise, categorical -> numeric\n",
        "# Replace 'NA' or 'NaN' as appropriate; Salary target might have missing values -> drop those rows\n",
        "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
        "df = df.dropna(subset=['Salary']).reset_index(drop=True)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "# In this dataset, common categorical cols: 'League','Division','NewLeague' (plus player names)\n",
        "# Remove player name column if present (it might be first column)\n",
        "# If the first column is a player name, drop it\n",
        "if df.columns[0].lower() not in [c.lower() for c in ['AtBat','Hits','HmRun','Runs','RBI']]:\n",
        "    df = df.iloc[:,1:]  # drop name column\n",
        "\n",
        "# Re-evaluate cat columns\n",
        "cat_cols = [c for c in df.select_dtypes(include=['object']).columns if c not in ['Player','Name']]\n",
        "print(\"Categorical columns:\", cat_cols)\n",
        "\n",
        "# (b) Separate input/output and perform scaling\n",
        "X = df.drop(columns=['Salary'])\n",
        "y = df['Salary'].values\n",
        "\n",
        "# Build ColumnTransformer: One-hot encode categorical, passthrough numerical, then scale\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# (c) Fit Linear, Ridge, Lasso (alpha=0.5748 for Ridge and Lasso)\n",
        "alpha_val = 0.5748\n",
        "\n",
        "models = {\n",
        "    'Linear': Pipeline([('pre', preprocessor), ('lr', LinearRegression())]),\n",
        "    'Ridge' : Pipeline([('pre', preprocessor), ('rg', Ridge(alpha=alpha_val))]),\n",
        "    'Lasso' : Pipeline([('pre', preprocessor), ('ls', Lasso(alpha=alpha_val, max_iter=10000))])\n",
        "}\n",
        "\n",
        "for name, pipe in models.items():\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    print(f\"{name}: RMSE={rmse:.3f}, R2={r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43e6cuIWUaBQ"
      },
      "source": [
        "Cross Validation for Ridge and Lasso Regression\n",
        "\n",
        "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
        "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
        "dataset from sklearn.datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dtOU6rIUJKz",
        "outputId": "b81da082-c6c7-42b3-bf2e-e02df81622bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RidgeCV best alpha: 26.126752255633264\n",
            "RidgeCV R2: 0.5211263118777945\n",
            "RidgeCV MSE: 20.07762625463206\n",
            "LassoCV best alpha: 0.2576340575560743\n",
            "LassoCV R2: 0.5342871059328007\n",
            "LassoCV MSE: 19.52583668923175\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "try:\n",
        "    from sklearn.datasets import load_boston\n",
        "    b = load_boston()\n",
        "    X = b.data\n",
        "    y = b.target\n",
        "except Exception:\n",
        "    url = r\"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "    raw = pd.read_csv(url, sep=r\"\\s+\", header=None, skiprows=22)\n",
        "    data = np.hstack([raw.values[::2, :], raw.values[1::2, :2]])\n",
        "    X = data[:, :-1]\n",
        "    y = data[:, -1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_s = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_s, y, test_size=0.2, random_state=0)\n",
        "\n",
        "alphas = np.logspace(-6, 6, 200)\n",
        "ridge_cv = RidgeCV(alphas=alphas, store_cv_results=True)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "preds_ridge = ridge_cv.predict(X_test)\n",
        "print(\"RidgeCV best alpha:\", ridge_cv.alpha_)\n",
        "print(\"RidgeCV R2:\", r2_score(y_test, preds_ridge))\n",
        "print(\"RidgeCV MSE:\", mean_squared_error(y_test, preds_ridge))\n",
        "\n",
        "lasso_cv = LassoCV(cv=5, max_iter=10000)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "preds_lasso = lasso_cv.predict(X_test)\n",
        "print(\"LassoCV best alpha:\", lasso_cv.alpha_)\n",
        "print(\"LassoCV R2:\", r2_score(y_test, preds_lasso))\n",
        "print(\"LassoCV MSE:\", mean_squared_error(y_test, preds_lasso))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j5MM78OVQ9e"
      },
      "source": [
        "Multiclass Logistic Regression:\n",
        "Implement Multiclass Logistic Regression (step-by step) on Iris dataset using one vs. rest strategy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx268UYAU7lG",
        "outputId": "d70798f5-05d5-49d3-988a-18c44c69b465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_s = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_s, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_binary_logistic(X, y_bin, lr=0.1, n_iter=5000, tol=1e-6):\n",
        "    m, d = X.shape\n",
        "    Xb = np.hstack([np.ones((m,1)), X])\n",
        "    w = np.zeros(d+1)\n",
        "    for _ in range(n_iter):\n",
        "        preds = sigmoid(Xb @ w)\n",
        "        grad = (1/m) * Xb.T @ (preds - y_bin)\n",
        "        w -= lr * grad\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "    return w\n",
        "\n",
        "K = len(np.unique(y_train))\n",
        "weights = []\n",
        "for k in range(K):\n",
        "    y_bin = (y_train == k).astype(int)\n",
        "    w_k = train_binary_logistic(X_train, y_bin, lr=0.3, n_iter=10000)\n",
        "    weights.append(w_k)\n",
        "weights = np.vstack(weights)\n",
        "\n",
        "def predict_ovr(X, weights):\n",
        "    Xb = np.hstack([np.ones((X.shape[0],1)), X])\n",
        "    probs = sigmoid(Xb @ weights.T)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "y_pred = predict_ovr(X_test, weights)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
